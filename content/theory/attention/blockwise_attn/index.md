---
title: Blockwise Attention
type: docs
weight: 60
---

åœ¨å®é™…è¿è¡Œæ¨¡å‹æ—¶ï¼Œå¦‚æœè¾“å…¥çš„åºåˆ—è¿‡é•¿ï¼Œå¯èƒ½éœ€è¦å°†åºåˆ—åˆ‡åˆ†æˆè‹¥å¹²æ®µï¼Œåˆ†åˆ«è®¡ç®—åå†åˆå¹¶ç»“æœã€‚è¿™ç§æƒ…å†µä¸‹å°±ä¼šç”¨åˆ°åˆ†æ®µ attention æŠ€æœ¯ã€‚è¯¥æŠ€æœ¯ä¸»è¦ç”¨äº chunked prefill å’Œ SP å¹¶è¡Œç­‰æŠ€æœ¯ï¼Œæœ¬èŠ‚å°†é‡ç‚¹ä»‹ç»å…¶èƒŒåçš„æ•°å­¦åŸç†ã€‚

### Blockwise Softmax

#### LSE

ä¸ºäº†æ›´å¥½åœ°è¿›è¡Œåˆ†æ®µè®¡ç®—ï¼Œæˆ‘ä»¬å¼•å…¥ä¸€ä¸ªé‡è¦çš„æ•°å­¦å·¥å…·ï¼š**å¯¹æ•°æ±‚å’ŒæŒ‡æ•°**ï¼ˆLog-Sum-Expï¼Œç®€ç§° LSEï¼‰ï¼Œå…¶å®šä¹‰ä¸ºï¼š

\[
\text{LSE}(x_1, x_2, \dots, x_n) = \log\left( \sum_{i=1}^{n} e^{x_i} \right)
\]

åœ¨è®¡ç®— softmax æ—¶ï¼Œä¸ºäº†é¿å…æ•°å€¼è¿‡å¤§ï¼ˆä¸Šæº¢å‡ºï¼‰æˆ–è¿‡å°ï¼ˆä¸‹æº¢å‡ºï¼‰ï¼Œé€šå¸¸ä¼šå…ˆå‡å»åºåˆ—ä¸­çš„æœ€å¤§å€¼ \( M = \max(x) \)ï¼Œå³ï¼š

\[
\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}} = \frac{e^{x_i - M}}{\sum_{j=1}^{n} e^{x_j - M}}
\]

åˆ©ç”¨ LSEï¼Œæˆ‘ä»¬å¯ä»¥å°† softmax å†™ä¸ºæ›´ç¨³å®šçš„å½¢å¼ã€‚é¦–å…ˆè®¡ç®—ï¼š

\[
\text{LSE}(x) = M + \log\left( \sum_{i=1}^{n} e^{x_i - M} \right)
\]

è¿™æ ·ï¼Œsoftmax å°±å¯ä»¥è¡¨ç¤ºä¸ºï¼š

\[
\text{softmax}(x_i) = e^{x_i - \text{LSE}(x)}
\]

è¿™ç§æ–¹æ³•æ—¢é¿å…äº†ä¸Šæº¢å‡ºï¼Œä¹Ÿä¿è¯äº†ä¸‹æº¢å‡ºæ—¶è‡³å°‘æœ‰ä¸€é¡¹å€¼ä¸º 1ã€‚

æ­¤å¤–ï¼ŒLSE æœ‰ä¸€ä¸ªå¾ˆæœ‰ç”¨çš„æ€§è´¨ï¼š**å®ƒå¯¹ \( x_i \) çš„å¯¼æ•°æ­£å¥½æ˜¯ softmax**ï¼š

\[
\frac{\partial}{\partial x_i} \log\left( \sum_{j=1}^{n} e^{x_j} \right) = \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}} = \text{softmax}(x_i)
\]

#### åˆ†æ®µè®¡ç®—çš„æ–¹æ³•

åœ¨å®é™…è®¡ç®—ä¸­ï¼Œå¦‚æœå‘é‡ \( X \) å¾ˆé•¿ï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶åˆ†æˆè‹¥å¹²æ®µä¾æ¬¡å¤„ç†ã€‚å‡è®¾å°† \( X \) åˆ†ä¸º \( n \) æ®µï¼š

\[
X = [X_1, X_2, \dots, X_n]
\]

**ç¬¬ä¸€æ­¥ï¼šå¤„ç†ç¬¬ä¸€æ®µ \( X_1 \)**

æˆ‘ä»¬åªèƒ½åŸºäºå½“å‰æ®µçš„ä¿¡æ¯è®¡ç®—ï¼š

\[
\text{lse}_1 = \log\left( \sum_{x \in X_1} e^{x} \right)
\]
\[
\text{softmax}_1(x_i) = e^{x_i - \text{lse}_1}, \quad x_i \in X_1
\]

**ç¬¬äºŒæ­¥ï¼šå¤„ç†ç¬¬äºŒæ®µ \( X_2 \)**

å½“è¯»åˆ°ç¬¬äºŒæ®µæ—¶ï¼Œæˆ‘ä»¬éœ€è¦æ›´æ–°ä¹‹å‰çš„ç»“æœã€‚è®¡ç®—ç¬¬äºŒæ®µçš„ LSEï¼š

\[
\text{lse}_2 = \log\left( \sum_{x \in X_2} e^{x} \right)
\]

æ­¤æ—¶ï¼Œå‰ä¸¤æ®µåˆå¹¶åçš„ softmax å¯ä»¥è¡¨ç¤ºä¸ºï¼š

\[
\text{softmax}'(x_i) = \frac{e^{x_i}}{e^{\text{lse}_1} + e^{\text{lse}_2}} = \frac{e^{x_i - \text{lse}_1}}{1 + e^{\text{lse}_2 - \text{lse}_1}}, \quad x_i \in [X_1, X_2]
\]

è¿™ç­‰ä»·äºï¼š

\[
\text{softmax}'(x_i) = \text{softmax}_1(x_i) \cdot \frac{1}{1 + e^{\text{lse}_2 - \text{lse}_1}}
\]

è‹¥è®° \( \sigma(z) = \frac{1}{1 + e^{-z}} \) ä¸º sigmoid å‡½æ•°ï¼Œåˆ™ä¸Šå¼å¯å†™ä¸ºï¼š

\[
\text{softmax}'(x_i) = \text{softmax}_1(x_i) \cdot \sigma(\text{lse}_1 - \text{lse}_2)
\]

#### é€’æ¨å…¬å¼

è®¾è¾“å…¥å‘é‡ \( X \) è¢«åˆ†ä¸º \( n \) æ®µï¼š  
\[
X = [X_1, X_2, \dots, X_n]
\]  
è®°ç¬¬ \( k \) æ®µçš„æ•°æ®ä¸º \( X_k \)ï¼Œå¹¶å®šä¹‰ï¼š

- \( \text{lse}^{(k)} \)ï¼šå‰ \( k \) æ®µåˆå¹¶åçš„ **log-sum-exp** å€¼  
\[
\text{lse}^{(k)} = \log\left( \sum_{j=1}^k \sum_{x \in X_j} e^x \right)
\]
- \( \text{softmax}^{(k)}(x_i) \)ï¼šå‰ \( k \) æ®µåˆå¹¶åï¼Œå¯¹æŸä¸ªå…ƒç´  \( x_i \)ï¼ˆå±äºå‰ \( k \) æ®µï¼‰çš„ softmax å€¼ã€‚

**åˆå§‹æ®µï¼ˆ\( k=1 \)ï¼‰**  
\[
\text{lse}^{(1)} = \log\left( \sum_{x \in X_1} e^x \right)
\]
\[
\text{softmax}^{(1)}(x_i) = e^{x_i - \text{lse}^{(1)}}, \quad x_i \in X_1
\]

**é€’æ¨å…¬å¼ï¼ˆä» \( k-1 \) æ®µåˆ° \( k \) æ®µï¼‰**  

1. **è®¡ç®—ç¬¬ \( k \) æ®µçš„å±€éƒ¨ LSE**  
\[
\text{lse}_k^{\text{local}} = \log\left( \sum_{x \in X_k} e^x \right)
\]

2. **æ›´æ–°åˆå¹¶åçš„ LSE**  
\[
\text{lse}^{(k)} = \log\left( e^{\text{lse}^{(k-1)}} + e^{\text{lse}_k^{\text{local}}} \right)
\]  
æˆ–è€…ç­‰ä»·åœ°ï¼ˆæ•°å€¼æ›´ç¨³å®šå½¢å¼ï¼‰ï¼š  
\[
\text{lse}^{(k)} = \max(a,b) + \log\left( e^{a - \max(a,b)} + e^{b - \max(a,b)} \right)
\]  
å…¶ä¸­ \( a = \text{lse}^{(k-1)},\; b = \text{lse}_k^{\text{local}} \)ã€‚

3. **æ›´æ–° softmax å€¼ï¼ˆå¯¹ä¹‹å‰æ‰€æœ‰å…ƒç´ é‡æ–°ç¼©æ”¾ï¼‰**  
å¯¹äºä»»æ„ \( x_i \) å±äºå‰ \( k-1 \) æ®µï¼š  
\[
\text{softmax}^{(k)}(x_i) = \text{softmax}^{(k-1)}(x_i) \cdot \frac{e^{\text{lse}^{(k-1)}}}{e^{\text{lse}^{(k-1)}} + e^{\text{lse}_k^{\text{local}}}}
\]  
åˆ©ç”¨ sigmoid å‡½æ•° \( \sigma(z) = \frac{1}{1+e^{-z}} \)ï¼Œä¸Šå¼å¯å†™ä¸ºï¼š  
\[
\text{softmax}^{(k)}(x_i) = \text{softmax}^{(k-1)}(x_i) \cdot \sigma\!\left( \text{lse}^{(k-1)} - \text{lse}_k^{\text{local}} \right)
\]

4. **ç¬¬ \( k \) æ®µå†…å…ƒç´ çš„ softmax**  
å¯¹äº \( x_i \in X_k \)ï¼š  
\[
\text{softmax}^{(k)}(x_i) = e^{x_i - \text{lse}_k^{\text{local}}} \cdot \frac{e^{\text{lse}_k^{\text{local}}}}{e^{\text{lse}^{(k-1)}} + e^{\text{lse}_k^{\text{local}}}}
\]  
å³  
\[
\text{softmax}^{(k)}(x_i) = e^{x_i - \text{lse}_k^{\text{local}}} \cdot \sigma\!\left( \text{lse}_k^{\text{local}} - \text{lse}^{(k-1)} \right)
\]

**æ€»ç»“é€’æ¨å…³ç³»**  
\[
\boxed{
\begin{aligned}
\text{lse}^{(k)} &= \log\!\left( e^{\text{lse}^{(k-1)}} + e^{\text{lse}_k^{\text{local}}} \right), \\
\text{softmax}^{(k)}(x_i) &= 
\begin{cases}
\text{softmax}^{(k-1)}(x_i) \cdot \sigma\!\left( \text{lse}^{(k-1)} - \text{lse}_k^{\text{local}} \right), & x_i \in \bigcup_{j=1}^{k-1} X_j \\[6pt]
e^{x_i - \text{lse}_k^{\text{local}}} \cdot \sigma\!\left( \text{lse}_k^{\text{local}} - \text{lse}^{(k-1)} \right), & x_i \in X_k
\end{cases}
\end{aligned}
}
\]  
å…¶ä¸­ \( \text{lse}_k^{\text{local}} = \log\sum_{x \in X_k} e^x \)ï¼Œä¸” \( \text{lse}^{(1)} = \text{lse}_1^{\text{local}} \)ã€‚

### KV Chunked Attention

å…³äºâ€œKV chunked attentionâ€å’Œâ€œQ chunked attentionâ€è¿™ä¸¤ä¸ªæœ¯è¯­ï¼Œç›®å‰å­¦æœ¯ç•Œå¹¶æ²¡æœ‰ç»Ÿä¸€çš„å®šä¹‰ï¼Œä¸åŒæ–‡çŒ®æˆ–è®¨è®ºä¸­å¸¸å¸¸å‡ºç°åç§°æ··ç”¨ï¼Œå®¹æ˜“é€ æˆç†è§£ä¸Šçš„æ··æ·†ã€‚ä¸ºäº†ä¾¿äºåç»­è®¨è®ºï¼Œæˆ‘å…ˆå¯¹å®ƒä»¬åšä¸€ä¸ªç®€å•çš„åŒºåˆ†è¯´æ˜ï¼š

- **KV chunked attention**ï¼š  
  åœ¨è¿™ç§æ–¹æ³•ä¸­ï¼ŒæŸ¥è¯¢ï¼ˆQï¼‰æ˜¯å·²çŸ¥ä¸”å®Œæ•´çš„ï¼Œä½†ç”±äºè®¡å†…å­˜é™åˆ¶æˆ–å…¶ä»–åŸå› ï¼Œæ— æ³•ä¸€æ¬¡æ€§å°† Q ä¸æ‰€æœ‰å†å²çš„ KV è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—ã€‚å› æ­¤ï¼Œå°† KV åˆ‡åˆ†æˆå¤šä¸ªå°å—ï¼Œé€å—ä¸ Q è¿›è¡Œè®¡ç®—ï¼Œæœ€ååˆå¹¶å„å—çš„ç»“æœï¼Œä»è€Œå¾—åˆ°å®Œæ•´çš„æ³¨æ„åŠ›è¾“å‡ºã€‚

- **Q chunked attention**ï¼š  
  è¿™ç§æ–¹æ³•åˆ™æ˜¯å°†æŸ¥è¯¢ï¼ˆQï¼‰åˆ‡åˆ†æˆå¤šä¸ªå—ï¼Œç„¶åé€å—ä¸å®Œæ•´çš„é”®å€¼ï¼ˆKVï¼‰è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—ï¼Œæœ€åå†åˆå¹¶ç»“æœã€‚

---

è¿™é‡Œé¦–å…ˆéœ€è¦è¯´æ˜ kv chunked prefillï¼Œå†ä¸Šæ–‡ä¸­æˆ‘ä»¬æåˆ°çš„ \(Q\) å¯ä»¥æ˜¯è‹¥å¹²ä¸ª token çš„ \(q\) ç»„æˆã€‚ä¸ºäº† æ–¹ä¾¿è®ºè¯ï¼Œæˆ‘ä»¬å…ˆä»ä¸€ä¸ªç®€å•çš„ case å‡ºå‘ï¼Œé¦–å…ˆ **å†»ç»“æ‰€æœ‰è‡ªç”±åº¦**ï¼šå½“å‰åªç®—ä¸€ä¸ª token çš„ \(q\)

{{< region note >}}
åœ¨æ•´ä¸ªæ¨å¯¼é‡Œï¼š

* **å”¯ä¸€çš„â€œè‡ªå˜é‡â€æ˜¯å½“å‰çš„ query å‘é‡ \( q_x \)**
* **\(K, V\) æ˜¯å·²çŸ¥å¸¸é‡ï¼ˆKV cacheï¼‰**
* **\(A_i, B_i, \text{attn}_i, \text{LSE}_i\)**
  ğŸ‘‰ **å…¨éƒ¨éƒ½æ˜¯â€œä¸­é—´è®¡ç®—ç»“æœ / æ ‡é‡æˆ–å‘é‡å€¼â€**

**æ²¡æœ‰ä»»ä½•éšæœºå˜é‡ã€æ²¡æœ‰å‡½æ•°æœªå®šé¡¹**
{{< /region >}}

æˆ‘ä»¬æ­£åœ¨è®¡ç®—çš„æ˜¯ï¼š

\[
\text{attention}(q_x, K, V)
\]

è¿™é‡Œï¼š

* \(x\)ï¼šquery çš„ä½ç½®ï¼ˆtoken indexï¼‰
* **\(q_x\)ï¼šä¸€ä¸ªç¡®å®šçš„å‘é‡**

**\(K, V\) æ˜¯å·²çŸ¥çš„ã€ä½†å¤ªå¤§ â†’ è¢«åˆ†å—**

å‡è®¾ï¼š

* åºåˆ—é•¿åº¦ = \(\text{seqlen}\)
* è¢«æ‹†æˆ \(B_{KV}\) ä¸ª blockï¼š

\[
K = [K_1; K_2; \dots; K_{B_{KV}}], \quad
V = [V_1; V_2; \dots; V_{B_{KV}}]
\]

è¿™äº›éƒ½æ˜¯ **å·²çŸ¥å¸¸é‡çŸ©é˜µ**

---

**åŸå§‹ attentionï¼ˆä¸æ‹†åˆ†ï¼‰**

\[
\text{attn}(q_x) =
\frac{\sum_{y=1}^{\text{seqlen}} e^{w_{xy}} v_y}
{\sum_{y=1}^{\text{seqlen}} e^{w_{xy}}}
\]

**KV æ‹†åˆ†ä¹‹å**

æˆ‘ä»¬åªæ˜¯æŠŠ **åŒä¸€ä¸ªæ±‚å’Œ** æ‹†æˆå‡ æ®µè€Œå·²ï¼š

å¯¹ **ç¬¬ (i) ä¸ª KV block**ï¼š

\[
\boxed{
\begin{aligned}
B_i &= \sum_{y \in \text{block } i} e^{q_x k_y^T} \quad\text{ï¼ˆæ ‡é‡ï¼‰}\\[4pt]
A_i &= \sum_{y \in \text{block } i} e^{q_x k_y^T} v_y \quad\text{ï¼ˆå‘é‡ï¼‰}
\end{aligned}}
\]

ğŸ‘‰ æ³¨æ„ï¼š

| ç¬¦å·         | ç±»å‹                |
| ---------- | ----------------- |
| \(q_x\)      | å›ºå®šå‘é‡              |
| \(K_i, V_i\) | å›ºå®šçŸ©é˜µ              |
| \(B_i\)      | **ç®—å‡ºæ¥çš„æ ‡é‡**        |
| \(A_i\)      | **ç®—å‡ºæ¥çš„å‘é‡ï¼ˆ\(d_v\) ç»´ï¼‰** |

**å®Œæ•´ attention å°±æ˜¯**

\[
\text{attn}
= \frac{\sum_i A_i}{\sum_i B_i}
\]

å®šä¹‰ï¼š

\[
\boxed{\text{attn}_i := \frac{A_i}{B_i}}
\]

å®ƒè¡¨ç¤ºï¼š**â€œåªçœ‹ç¬¬ i ä¸ª KV blockï¼Œç®—å‡ºæ¥çš„å±€éƒ¨ attentionâ€**

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†è¯´æ˜å¦‚ä½•é€šè¿‡å­æ¨¡å—å¯¼å‡ºå…¨å±€æ³¨æ„åŠ›ï¼ˆAttnï¼‰çš„ç»“æœã€‚é¦–å…ˆä»ç®€å•æƒ…å†µå…¥æ‰‹ï¼Œå‡è®¾å°†å…¨å±€æ³¨æ„åŠ›åˆ†æˆä¸¤ä¸ªå­å—å¤„ç†ï¼š

é‚£ä¹ˆï¼Œå°†è¿™ä¸¤ä¸ªå­å—åˆå¹¶åçš„å…¨å±€æ³¨æ„åŠ›å¯è¡¨ç¤ºä¸ºï¼š

\[
\text{Attn} = \frac{A_1 + A_2}{B_1 + B_2}
\]

ä»£å…¥ \(A_i = \text{attn}_i \cdot B_i\)ï¼š

\[
\begin{aligned}
\text{attn}_{12}
&= \frac{\text{attn}_1 B_1 + \text{attn}_2 B_2}{B_1 + B_2} \
&= \text{attn}_1 \frac{B_1}{B_{12}} + \text{attn}_2 \frac{B_2}{B_{12}}
\end{aligned}
\]

\[
\text{LSE}_i := \log B_i
\]

---

**åˆå¹¶ä¸¤ä¸ª block çš„ LSEï¼š**

\[
\begin{aligned}
\text{LSE}_{12}
&= \log(B_1 + B_2) \
&= \log\left(e^{\text{LSE}_1} + e^{\text{LSE}_2}\right) \
&= \text{LSE}_1 + \log\left(1 + e^{\text{LSE}_2 - \text{LSE}_1}\right)
\end{aligned}
\]

è¿™å°±æ˜¯ **log-sum-exp trick**

---

**attn çš„åŠ æƒä¹Ÿç”¨ LSE è¡¨è¾¾ï¼š**

\[
\frac{B_i}{B_{12}} = e^{\text{LSE}_i - \text{LSE}_{12}}
\]

äºæ˜¯ï¼š

\[
\boxed{
\text{attn}_{12}
= \text{attn}_1 e^{\text{LSE}_1 - \text{LSE}_{12}} + \text{attn}_2 e^{\text{LSE}_2 - \text{LSE}_{12}}
}
\]

---

äº‹å®ä¸Šè¿˜å¯ä»¥è¿›ä¸€æ­¥ç®€åŒ–ï¼Œå³ä¸éœ€è¦è®¡ç®—å‡º \(LSE_{12}\)ï¼Œè€Œæ”¹ç”¨ sigmoid å‡½æ•°å®ç°æ¨å¯¼å¦‚ä¸‹ï¼š

\[
\begin{aligned}
\text{attn}_{12} &= \text{attn}_1 \cdot e^{LSE_1 - LSE_{12}} + \text{attn}_2 \cdot e^{LSE_2 - LSE_{12}} \\
&= \text{attn}_1 \cdot e^{-\log(1 + e^{LSE_2 - LSE_1})} + \text{attn}_2 \cdot e^{LSE_2 - LSE_1 - \log(1 + e^{LSE_2 - LSE_1})} \\
&= \text{attn}_1 \cdot \frac{1}{1 + e^{LSE_2 - LSE_1}} + \text{attn}_2 \cdot \frac{e^{LSE_2 - LSE_1}}{1 + e^{LSE_2 - LSE_1}} \\
&= \text{attn}_1 \cdot \frac{1}{1 + e^{LSE_2 - LSE_1}} + \text{attn}_2 \cdot \frac{1}{1 + e^{-LSE_2 + LSE_1}} \\
&= \text{attn}_1 \cdot \sigma(LSE_2 - LSE_1) + \text{attn}_2 \cdot \sigma(LSE_1 - LSE_2) \\
&= \text{attn}_1 - (\text{attn}_1 - \text{attn}_2) \cdot \sigma(LSE_1 - LSE_2)
\end{aligned}
\]

å…¶ä¸­ \(\sigma(x) = \dfrac{1}{1 + e^{-x}}\) ä¸º sigmoid å‡½æ•°ï¼Œå³

\[
\boxed{
\text{attn}_{12}
= \text{attn}_1 - (\text{attn}_1 - \text{attn}_2) \cdot \sigma(LSE_1 - LSE_2)
}
\]

> Streaming Attention çš„æœ¬è´¨
>
> * åªè¦èƒ½ç¨³å®šåœ°ç»´æŠ¤ï¼š
>
>   * å½“å‰ç´¯è®¡çš„ \( \text{LSE} \)
>   * å½“å‰ç´¯è®¡çš„ \( \text{attn} \)
>
> å°±å¯ä»¥ **ä¸€å—ä¸€å—åœ° streaming è®¡ç®— attention**

#### ä»£ç æè¿°

```python
def qk_chunked_attention(query, key_chunks, value_chunks):
    """
    åˆ†å—è®¡ç®— attention
    """
    # åˆå§‹åŒ–
    lse_global = -inf
    output = zeros_like(query @ values)
    
    for k_chunk, v_chunk in zip(key_chunks, value_chunks):
        # è®¡ç®—å½“å‰å—çš„ attention åˆ†æ•°
        scores = query @ k_chunk.T
        
        # è®¡ç®—å±€éƒ¨ LSE
        lse_local = logsumexp(scores, dim=-1)
        
        # æ›´æ–°å…¨å±€ LSE
        lse_global = logaddexp(lse_global, lse_local)
        
        # è®¡ç®—å½“å‰å—çš„ attention æƒé‡ï¼ˆéƒ¨åˆ† softmaxï¼‰
        attn_weights = exp(scores - lse_local)
        
        # è®¡ç®—å½“å‰å—çš„è´¡çŒ®
        chunk_output = attn_weights @ v_chunk
        
        # å¦‚æœæ˜¯ç¬¬ä¸€ä¸ªå—ï¼Œç›´æ¥ä½¿ç”¨
        # å¦‚æœä¸æ˜¯ç¬¬ä¸€ä¸ªå—ï¼Œéœ€è¦é‡æ–°ç¼©æ”¾ä¹‹å‰çš„è¾“å‡º
        if not first_chunk:
            # é‡æ–°ç¼©æ”¾ä¹‹å‰çš„è¾“å‡º
            scale = exp(lse_prev_global - lse_global)
            output *= scale
            
            # ç¼©æ”¾å½“å‰å—çš„è´¡çŒ®
            chunk_scale = exp(lse_local - lse_global)
            chunk_output *= chunk_scale
        
        # ç´¯åŠ å½“å‰å—çš„è´¡çŒ®
        output += chunk_output
        
        lse_prev_global = lse_global
    
    return output
```


### Q Chunked Attention

__chunk 1__

|       | \(k_0\) | \(k_1\) | \(k_2\) | \(k_3\) |
|-------|----|----|----|----|
| \(q_0\)    | 1  | -  | -  | -  |
| \(q_1\)    | 1  | 1  | -  | -  |
| \(q_2\)    | 1  | 1  | 1  | -  |
| \(q_3\)    | 1  | 1  | 1  | 1  |

__chunk 2__

|       | \(k_0\) | \(k_1\) | \(k_2\) | \(k_3\) | \(k_4\) | \(k_5\) | \(k_6\) | \(k_7\) |
|-------|----|----|----|----|----|----|----|----|
| \(q_4\)    | 1  | 1  | 1  | 1  | 1  | -  | -  | -  |
| \(q_5\)    | 1  | 1  | 1  | 1  | 1  | 1  | -  | -  |
| \(q_6\)    | 1  | 1  | 1  | 1  | 1  | 1  | 1  | -  |
| \(q_7\)    | 1  | 1  | 1  | 1  | 1  | 1  | 1  | 1  |

__chunk 3__ 

|       | \(k_0\) | \(k_1\) | \(k_2\) | \(k_3\) | \(k_4\) | \(k_5\) | \(k_6\) | \(k_7\) | \(k_8\) | \(k_9\) | \(k_{10}\) | \(k_{11}\) |
|-------|----|----|----|----|----|----|----|----|----|----|-----|-----|
| \(q_8\)    | 1  | 1  | 1  | 1  | 1  | 1  | 1  | 1  | 1  | -  | -   | -   |
| \(q_9\)    | 1  | 1  | 1  | 1  | 1  | 1  | 1  | 1  | 1  | 1  | -   | -   |
| \(q_{10}\)   | 1  | 1  | 1  | 1  | 1  | 1  | 1  | 1  | 1  | 1  | 1   | -   |
| \(q_{11}\)   | 1  | 1  | 1  | 1  | 1  | 1  | 1  | 1  | 1  | 1  | 1   | 1   |
